{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f77f199",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## **1. What Are Classification Metrics?**\n",
    "\n",
    "**Definition:**\n",
    "Classification metrics are quantitative measures used to evaluate how well a machine learning model predicts **categorical outcomes** (e.g., spam vs not spam, disease vs no disease).\n",
    "\n",
    "**Why They’re Important:**\n",
    "\n",
    "* They measure **how well the model distinguishes between different classes**.\n",
    "* They help identify whether the model is **accurate**, **balanced**, and **reliable**.\n",
    "* Different metrics focus on **different aspects** of performance — this is crucial when dealing with **imbalanced datasets** or **different costs for errors**.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. The Confusion Matrix – The Foundation**\n",
    "\n",
    "A **confusion matrix** summarizes predictions in a table:\n",
    "\n",
    "|                     | Predicted Positive      | Predicted Negative      |\n",
    "| ------------------- | ----------------------- | ----------------------- |\n",
    "| **Actual Positive** | **True Positive (TP)**  | **False Negative (FN)** |\n",
    "| **Actual Negative** | **False Positive (FP)** | **True Negative (TN)**  |\n",
    "\n",
    "* **TP:** Model correctly predicted positive.\n",
    "* **TN:** Model correctly predicted negative.\n",
    "* **FP:** Model predicted positive but was wrong (false alarm).\n",
    "* **FN:** Model predicted negative but missed a positive case.\n",
    "\n",
    "**Why it’s important:**\n",
    "Almost all classification metrics are derived from these four numbers.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Common Classification Metrics**\n",
    "\n",
    "---\n",
    "\n",
    "### **3.1 Accuracy**\n",
    "\n",
    "**Definition:**\n",
    "The proportion of correct predictions (both positives and negatives) out of all predictions.\n",
    "\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\n",
    "$$\n",
    "\n",
    "**Purpose:**\n",
    "\n",
    "* Quick measure of overall performance.\n",
    "\n",
    "**Interpretation:**\n",
    "\n",
    "* High accuracy means most predictions are correct.\n",
    "* **Limitation:** Misleading when classes are imbalanced (e.g., 95% accuracy predicting all \"Negative\" in a dataset with 95% negatives).\n",
    "\n",
    "**Example:**\n",
    "If TP = 50, TN = 40, FP = 5, FN = 5 → Accuracy = (50+40)/(50+40+5+5) = 90/100 = **0.90 (90%)**.\n",
    "\n",
    "**Best used when:** Classes are **balanced**.\n",
    "\n",
    "---\n",
    "\n",
    "### **3.2 Precision**\n",
    "\n",
    "**Definition:**\n",
    "The proportion of correctly predicted positives out of all predicted positives.\n",
    "\n",
    "$$\n",
    "\\text{Precision} = \\frac{TP}{TP + FP}\n",
    "$$\n",
    "\n",
    "**Purpose:**\n",
    "\n",
    "* Answers: **\"When the model predicts Positive, how often is it correct?\"**\n",
    "* Important in scenarios where **false positives are costly**.\n",
    "\n",
    "**Interpretation:**\n",
    "\n",
    "* High precision = low false alarm rate.\n",
    "* Low precision = many false positives.\n",
    "\n",
    "**Example:**\n",
    "If TP = 50, FP = 10 → Precision = 50/(50+10) = **0.83 (83%)**.\n",
    "\n",
    "**Best used when:** **False positives** are more harmful (e.g., predicting cancer when it’s not there → unnecessary anxiety/tests).\n",
    "\n",
    "---\n",
    "\n",
    "### **3.3 Recall (Sensitivity / True Positive Rate)**\n",
    "\n",
    "**Definition:**\n",
    "The proportion of actual positives correctly identified.\n",
    "\n",
    "$$\n",
    "\\text{Recall} = \\frac{TP}{TP + FN}\n",
    "$$\n",
    "\n",
    "**Purpose:**\n",
    "\n",
    "* Answers: **\"Out of all actual positives, how many did we correctly predict?\"**\n",
    "* Important in scenarios where **missing a positive case is costly**.\n",
    "\n",
    "**Interpretation:**\n",
    "\n",
    "* High recall = fewer missed cases.\n",
    "* Low recall = many false negatives.\n",
    "\n",
    "**Example:**\n",
    "If TP = 50, FN = 5 → Recall = 50/(50+5) = **0.91 (91%)**.\n",
    "\n",
    "**Best used when:** **False negatives** are more harmful (e.g., detecting diseases, fraud detection).\n",
    "\n",
    "---\n",
    "\n",
    "### **3.4 F1 Score**\n",
    "\n",
    "**Definition:**\n",
    "The **harmonic mean** of precision and recall.\n",
    "\n",
    "$$\n",
    "\\text{F1} = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "$$\n",
    "\n",
    "**Purpose:**\n",
    "\n",
    "* Balances precision and recall into a single number.\n",
    "* Useful when there’s **class imbalance** and you need to consider both false positives and false negatives.\n",
    "\n",
    "**Interpretation:**\n",
    "\n",
    "* F1 closer to 1 = better model.\n",
    "* Lower F1 = imbalance between precision and recall.\n",
    "\n",
    "**Example:**\n",
    "If Precision = 0.83 and Recall = 0.91 →\n",
    "F1 = 2 × (0.83 × 0.91) / (0.83 + 0.91) ≈ **0.87**.\n",
    "\n",
    "**Best used when:** You need **balance** between precision & recall.\n",
    "\n",
    "---\n",
    "\n",
    "### **3.5 ROC-AUC (Receiver Operating Characteristic – Area Under Curve)**\n",
    "\n",
    "**Definition:**\n",
    "\n",
    "* **ROC curve** plots True Positive Rate (Recall) vs False Positive Rate (FPR) for different thresholds.\n",
    "* **AUC** = Area under ROC curve (0 to 1).\n",
    "\n",
    "**Purpose:**\n",
    "\n",
    "* Measures **model’s ability to separate classes** regardless of threshold.\n",
    "* AUC = 0.5 → Random guessing.\n",
    "  AUC = 1.0 → Perfect classifier.\n",
    "\n",
    "**Interpretation:**\n",
    "\n",
    "* Higher AUC = better model.\n",
    "* Not affected by class imbalance as much as accuracy.\n",
    "\n",
    "**Example:**\n",
    "If a spam filter’s AUC is 0.95, it’s very good at separating spam from non-spam.\n",
    "\n",
    "**Best used when:** You want **threshold-independent** evaluation.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Choosing the Right Metric**\n",
    "\n",
    "| Scenario                           | Best Metric | Why                     |\n",
    "| ---------------------------------- | ----------- | ----------------------- |\n",
    "| Balanced classes                   | Accuracy    | Simple, intuitive       |\n",
    "| Cost of FP is high                 | Precision   | Avoid false alarms      |\n",
    "| Cost of FN is high                 | Recall      | Catch all positives     |\n",
    "| Need balance between FP and FN     | F1 Score    | Harmonic mean           |\n",
    "| Want threshold-independent measure | ROC-AUC     | Works across thresholds |\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Practical Example**\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score\n",
    "\n",
    "# Actual and predicted labels\n",
    "y_true = [1, 0, 1, 1, 0, 1, 0, 0, 1, 0]\n",
    "y_pred = [1, 0, 1, 0, 0, 1, 1, 0, 1, 0]\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Metrics\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "prec = precision_score(y_true, y_pred)\n",
    "rec = recall_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "auc = roc_auc_score(y_true, y_pred)\n",
    "\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "print(f\"Accuracy: {acc:.2f}\")\n",
    "print(f\"Precision: {prec:.2f}\")\n",
    "print(f\"Recall: {rec:.2f}\")\n",
    "print(f\"F1 Score: {f1:.2f}\")\n",
    "print(f\"ROC-AUC: {auc:.2f}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **6. Key Takeaways**\n",
    "\n",
    "* Always start with a **confusion matrix** — it’s the root of most metrics.\n",
    "* **Accuracy** is not always the best choice (especially with imbalanced data).\n",
    "* Pick metrics that align with **business goals**:\n",
    "\n",
    "  * If missing positives is costly → Focus on **Recall**.\n",
    "  * If false alarms are costly → Focus on **Precision**.\n",
    "  * If both matter → Use **F1 score**.\n",
    "  * If you want to compare across thresholds → Use **ROC-AUC**.\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
