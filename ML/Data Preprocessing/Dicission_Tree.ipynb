{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5834f7f8",
   "metadata": {},
   "source": [
    "# üå≥ Decision Tree Algorithm ‚Äî Complete Guide\n",
    "\n",
    "---\n",
    "\n",
    "## üìò What is a Decision Tree?\n",
    "\n",
    "A **Decision Tree** is a **supervised machine learning algorithm** used for both **classification** and **regression** tasks. It mimics **human decision-making** by using a series of **if-else** conditions to split data into smaller, more manageable parts.\n",
    "\n",
    "Each internal node represents a **decision based on a feature**, each branch represents the **outcome of that decision**, and each leaf node represents the **final output** (either a class or a value).\n",
    "\n",
    "---\n",
    "\n",
    "## üß± Elements of a Decision Tree\n",
    "\n",
    "| Element           | Description                                                            |\n",
    "| ----------------- | ---------------------------------------------------------------------- |\n",
    "| **Root Node**     | The topmost node; represents the first and most important decision.    |\n",
    "| **Internal Node** | A decision node that splits data into subsets based on a condition.    |\n",
    "| **Leaf Node**     | A terminal node that gives the final output; no further splitting.     |\n",
    "| **Branch/Edge**   | Connects nodes and represents the result of a decision/test condition. |\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è How Does a Decision Tree Work?\n",
    "\n",
    "1. Begin at the **root node**.\n",
    "2. Evaluate all features and choose the **best one to split** based on a metric.\n",
    "3. Apply the split condition and **divide the data** into subsets.\n",
    "4. Recursively repeat this process for each subset.\n",
    "5. Stop when:\n",
    "\n",
    "   * All samples in a node belong to the same class\n",
    "   * A stopping criterion is reached (like max depth or min samples)\n",
    "\n",
    "---\n",
    "\n",
    "## üìè Criteria for Splitting\n",
    "\n",
    "| Method                  | Used For       | Description                            |\n",
    "| ----------------------- | -------------- | -------------------------------------- |\n",
    "| **Gini Index**          | Classification | Measures impurity; lower is better.    |\n",
    "| **Entropy / Info Gain** | Classification | Measures randomness or uncertainty.    |\n",
    "| **Variance Reduction**  | Regression     | Measures reduction in target variance. |\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Benefits of Decision Trees\n",
    "\n",
    "* Easy to **visualize** and interpret.\n",
    "* Handles both **classification and regression** problems.\n",
    "* Effective for **non-linear** relationships.\n",
    "* Requires **minimal preprocessing** (no need to scale or normalize data).\n",
    "* Naturally handles **missing values** in features.\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Why Is It Important?\n",
    "\n",
    "* Serves as the foundation for ensemble models like **Random Forest** and **XGBoost**.\n",
    "* Useful for **quick prototyping** and generating **baseline models**.\n",
    "* Helps determine **feature importance** intuitively.\n",
    "\n",
    "---\n",
    "\n",
    "## üß™ Step-by-Step Process of Building a Decision Tree\n",
    "\n",
    "### 1Ô∏è‚É£ Start at the Root Node\n",
    "\n",
    "* Evaluate all features and pick the best split using Gini Index or Entropy.\n",
    "\n",
    "### 2Ô∏è‚É£ Split the Dataset\n",
    "\n",
    "* Divide the dataset into branches based on the chosen feature and threshold.\n",
    "\n",
    "### 3Ô∏è‚É£ Repeat the Process\n",
    "\n",
    "* Recursively apply splitting to each resulting subset.\n",
    "\n",
    "### 4Ô∏è‚É£ Apply Stopping Criteria\n",
    "\n",
    "* Stop if:\n",
    "\n",
    "  * All instances in a node belong to the same class\n",
    "  * The tree reaches the maximum depth\n",
    "  * A node contains fewer samples than the minimum required\n",
    "\n",
    "### 5Ô∏è‚É£ Assign Final Outputs\n",
    "\n",
    "* Leaf nodes output either the most frequent class (classification) or the mean value (regression).\n",
    "\n",
    "---\n",
    "\n",
    "## üíª Python Example: Classification Tree\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load dataset\n",
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "# Train model\n",
    "model = DecisionTreeClassifier(max_depth=3)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Visualize decision tree\n",
    "plt.figure(figsize=(12, 8))\n",
    "plot_tree(model, filled=True, feature_names=load_iris().feature_names)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
