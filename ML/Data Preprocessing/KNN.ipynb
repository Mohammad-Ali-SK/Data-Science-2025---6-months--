{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3e34d9e",
   "metadata": {},
   "source": [
    "## üìò K-Nearest Neighbors (KNN) - Step-by-Step Guide\n",
    "\n",
    "---\n",
    "\n",
    "### üîç What is KNN?\n",
    "\n",
    "**K-Nearest Neighbors (KNN)** is a simple, yet powerful **supervised machine learning algorithm** used for **classification** and **regression** problems.\n",
    "\n",
    "It classifies a data point based on how its **neighbors are classified**.\n",
    "\n",
    "---\n",
    "\n",
    "### üß† How KNN Works - Step-by-Step\n",
    "\n",
    "1. **Choose the number of neighbors (K)**\n",
    "\n",
    "   * K is the number of closest data points used to make the prediction.\n",
    "\n",
    "2. **Calculate the distance** between the test data and all training points.\n",
    "\n",
    "3. **Sort the distances** and identify the **K nearest neighbors**.\n",
    "\n",
    "4. **Vote for classes** (for classification) or **average the values** (for regression).\n",
    "\n",
    "5. **Assign the class or value** to the test data.\n",
    "\n",
    "---\n",
    "\n",
    "## üìè Distance Metrics in KNN\n",
    "\n",
    "### 1Ô∏è‚É£ **Euclidean Distance** (default and most common)\n",
    "\n",
    "$$\n",
    "D = \\sqrt{(x_1 - y_1)^2 + (x_2 - y_2)^2 + ... + (x_n - y_n)^2}\n",
    "$$\n",
    "\n",
    "* Measures the straight-line distance.\n",
    "* Best for continuous variables.\n",
    "\n",
    "### 2Ô∏è‚É£ **Manhattan Distance**\n",
    "\n",
    "$$\n",
    "D = |x_1 - y_1| + |x_2 - y_2| + ... + |x_n - y_n|\n",
    "$$\n",
    "\n",
    "* Measures distance in grid-like paths (L1 norm).\n",
    "* Suitable when features are not correlated.\n",
    "\n",
    "### 3Ô∏è‚É£ **Minkowski Distance**\n",
    "\n",
    "$$\n",
    "D = \\left(\\sum |x_i - y_i|^p \\right)^{1/p}\n",
    "$$\n",
    "\n",
    "* Generalization of Euclidean and Manhattan.\n",
    "* When **p = 1** ‚Üí Manhattan, **p = 2** ‚Üí Euclidean\n",
    "\n",
    "### 4Ô∏è‚É£ **Hamming Distance**\n",
    "\n",
    "* Used for categorical variables (like strings or binary).\n",
    "* Counts the number of positions with differing values.\n",
    "\n",
    "---\n",
    "\n",
    "## üß™ Choosing the Right K\n",
    "\n",
    "* **Low K (e.g., K=1)**:\n",
    "\n",
    "  * Highly sensitive to noise (overfitting).\n",
    "\n",
    "* **High K**:\n",
    "\n",
    "  * More generalized, but may underfit.\n",
    "\n",
    "### ‚öñÔ∏è Tip:\n",
    "\n",
    "* Use **cross-validation** to choose the best value of K.\n",
    "\n",
    "---\n",
    "\n",
    "## üîß KNN in Python (Classification Example)\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load data\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# KNN model\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üìà Pros and Cons\n",
    "\n",
    "### ‚úÖ Pros:\n",
    "\n",
    "* Simple and easy to implement\n",
    "* No assumptions about data distribution\n",
    "* Effective with small datasets\n",
    "\n",
    "### ‚ùå Cons:\n",
    "\n",
    "* Slow with large datasets (needs to compute distance for all points)\n",
    "* Sensitive to irrelevant or scaled features\n",
    "* Doesn‚Äôt work well with high-dimensional data (curse of dimensionality)\n",
    "\n",
    "---\n",
    "\n",
    "## üìä When to Use KNN?\n",
    "\n",
    "* You need a **baseline classifier**\n",
    "* Data is **low-dimensional**\n",
    "* Problem is **non-linear and you want flexibility**\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Summary Table\n",
    "\n",
    "| Feature          | Description                              |\n",
    "| ---------------- | ---------------------------------------- |\n",
    "| Algorithm Type   | Supervised (classification & regression) |\n",
    "| Model Type       | Instance-based (lazy learner)            |\n",
    "| Distance Metrics | Euclidean, Manhattan, Minkowski, Hamming |\n",
    "| Best K           | Determined via cross-validation          |\n",
    "| Good For         | Small datasets, non-linear relationships |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878a41e4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4a517837",
   "metadata": {},
   "source": [
    "# Euclidean Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b2e9c7",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
